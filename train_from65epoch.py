# train_from65epoch_fixed.py
import os
# [ì¤‘ìš”] 0ë²ˆ, 1ë²ˆ GPUë§Œ ë³´ì´ê²Œ ì„¤ì • (ì½”ë“œ ìµœìƒë‹¨ ìœ„ì¹˜)
os.environ["CUDA_VISIBLE_DEVICES"] = "0,1"

import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from torch.amp import autocast, GradScaler
from torch.optim.lr_scheduler import CosineAnnealingLR

from src.config import Config
from src.model import FlowMatchingTransformer, AnnealedPseudoHuberLoss
from src.dataset import EGMDDataset
from src.utils import seed_everything
from tqdm import tqdm
import math

# Gradient Clippingì„ ìœ„í•œ max norm (Flow Matchingì— ì í•©í•˜ê²Œ ì¡°ì •)
MAX_GRAD_NORM = 1.0  # [ìˆ˜ì •] 0.5 -> 1.0ìœ¼ë¡œ ì¦ê°€ (ìƒì„±ëª¨ë¸ì— ì í•©)

def check_for_nan(tensor, name):
    """NaN ì²´í¬ ìœ í‹¸ë¦¬í‹°"""
    if torch.isnan(tensor).any():
        print(f"[WARNING] NaN detected in {name}!")
        return True
    if torch.isinf(tensor).any():
        print(f"[WARNING] Inf detected in {name}!")
        return True
    return False

def create_lr_scheduler(optimizer, total_epochs, warmup_epochs=5):
    """
    Learning Rate Scheduler: Warmup -> Peak -> Cosine Decay
    - Warmup: 5e-5 -> 1e-4 (5 epochs)
    - Peak: 1e-4 (20 epochs)  
    - Cosine Decay: 1e-4 -> 1e-6 (remaining epochs)
    """
    def lr_lambda(epoch):
        if epoch < warmup_epochs:
            # Warmup phase: 0.5 -> 1.0
            return 0.5 + 0.5 * (epoch / warmup_epochs)
        elif epoch < warmup_epochs + 20:
            # Peak phase: maintain 1.0
            return 1.0
        else:
            # Cosine decay phase
            remaining_epochs = total_epochs - warmup_epochs - 20
            progress = (epoch - warmup_epochs - 20) / remaining_epochs
            return 0.01 + 0.99 * (1 + math.cos(math.pi * progress)) / 2
    
    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)

def load_checkpoint_full(model, optimizer, scheduler, checkpoint_path, device):
    """ì²´í¬í¬ì¸íŠ¸ ì™„ì „ ë³µì› (ëª¨ë¸ + optimizer + scheduler)"""
    print(f"Loading checkpoint from {checkpoint_path}...")
    checkpoint = torch.load(checkpoint_path, map_location=device)
    
    # DataParallel wrapper ê³ ë ¤
    if isinstance(model, nn.DataParallel):
        model.module.load_state_dict(checkpoint['model_state_dict'])
    else:
        model.load_state_dict(checkpoint['model_state_dict'])
    
    # Optimizer ìƒíƒœ ë³µì› (ìˆëŠ” ê²½ìš°)
    if 'optimizer_state_dict' in checkpoint and optimizer is not None:
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        print("âœ… Optimizer ìƒíƒœ ë³µì›ë¨")
    else:
        print("âš ï¸ Optimizer ìƒíƒœ ì—†ìŒ - ì²˜ìŒë¶€í„° ì‹œì‘")
    
    # Scheduler ìƒíƒœ ë³µì› (ìˆëŠ” ê²½ìš°) 
    if 'scheduler_state_dict' in checkpoint and scheduler is not None:
        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        print("âœ… Scheduler ìƒíƒœ ë³µì›ë¨")
    else:
        print("âš ï¸ Scheduler ìƒíƒœ ì—†ìŒ - ì²˜ìŒë¶€í„° ì‹œì‘")
    
    loaded_epoch = checkpoint.get('epoch', 65)
    loaded_loss = checkpoint.get('loss', 'Unknown')
    
    print(f"âœ… Checkpoint ì™„ì „ ë³µì› ì™„ë£Œ!")
    print(f"  - Epoch: {loaded_epoch}")
    print(f"  - Loss: {loaded_loss}")
    
    return loaded_epoch

def main():
    seed_everything(42)
    cfg = Config()
    # [ê°•ì œ ì„¤ì •] Effective Batch Size í™•ë³´ (12 * 6 = 72, ë…¼ë¬¸ ê¸°ì¤€ 64 ì´ìƒ)
    cfg.GRAD_ACCUM_STEPS = 6
    device = torch.device(cfg.DEVICE)
    
    # [ìˆ˜ì •] 65 epochë¶€í„° 150ê¹Œì§€ í•™ìŠµ
    TOTAL_EPOCHS = 150
    START_EPOCH = 65  # 65 epochë¶€í„° ì‹œì‘
    
    print(f"ğŸš€ Resume training from epoch {START_EPOCH+1} to {TOTAL_EPOCHS}...")
    
    print("Initializing Dataset...")
    dataset = EGMDDataset(is_train=True)
    dataloader = DataLoader(
        dataset, 
        batch_size=cfg.BATCH_SIZE, 
        shuffle=True, 
        num_workers=cfg.NUM_WORKERS,
        pin_memory=True
    )
    
    print("Initializing Model...")
    backbone = FlowMatchingTransformer(cfg).to(device)
    
    # [ì„¤ì •] GPU 2ê°œ ëª¨ë‘ ì‚¬ìš©
    if torch.cuda.device_count() > 1:
        print(f"Using {torch.cuda.device_count()} GPUs!")
        backbone = nn.DataParallel(backbone)
    
    # ìˆ˜ì •ëœ Loss Wrapper ì‚¬ìš©
    loss_wrapper = AnnealedPseudoHuberLoss(backbone, cfg).to(device)
    
    # [ìˆ˜ì •] Learning Rateë¥¼ 1e-4ë¡œ ì„¤ì • (peak LR)
    optimizer = torch.optim.AdamW(
        backbone.parameters(), 
        lr=1e-4,  # Peak learning rate
        weight_decay=0.01, 
        betas=(0.9, 0.999)  # [ìˆ˜ì •] 0.99 -> 0.999 (Flow Matching ì•ˆì •ì„± í–¥ìƒ)
    )
    
    # [í•µì‹¬] 65 epoch ì²´í¬í¬ì¸íŠ¸ ì™„ì „ ë³µì› (scheduler ì„¤ì • ì „ì— ë¨¼ì €)
    checkpoint_path = "checkpoints/n2n_from50_ep65.pth"
    if os.path.exists(checkpoint_path):
        # ì„ì‹œë¡œ ì „ì²´ ìŠ¤ì¼€ì¤„ëŸ¬ ìƒì„± (ì›ë˜ train_from50epoch.pyì™€ ë™ì¼í•˜ê²Œ)
        temp_remaining_epochs = TOTAL_EPOCHS - 50  # 50ë¶€í„° 150ê¹Œì§€ì˜ ì›ë˜ ìŠ¤ì¼€ì¤„
        temp_scheduler = create_lr_scheduler(optimizer, temp_remaining_epochs, warmup_epochs=5)  # ì›ë˜ ì„¤ì •
        loaded_epoch = load_checkpoint_full(backbone, optimizer, temp_scheduler, checkpoint_path, device)
        if loaded_epoch != 66:  # 66ë¶€í„° ì‹œì‘í•´ì•¼ í•¨ (65 ì™„ë£Œ í›„)
            print(f"âš ï¸ Warning: Expected epoch 66, got {loaded_epoch}")
        # ì‹¤ì œ ì‹œì‘ epoch ì¡°ì •
        START_EPOCH = max(loaded_epoch - 1, 65)  # 65 ì´ìƒì—ì„œ ì‹œì‘
        scheduler = temp_scheduler  # ë³µì›ëœ ì›ë˜ ìŠ¤ì¼€ì¤„ëŸ¬ ì‚¬ìš©
    else:
        raise FileNotFoundError(f"Checkpoint not found: {checkpoint_path}")
    
    scaler = GradScaler('cuda')  # PyTorch 2.x í˜¸í™˜
    
    print(f"Initial LR: {optimizer.param_groups[0]['lr']:.2e}")
    
    # [ìˆ˜ì •] Progress ê³„ì‚°ì„ ì‹¤ì œ ì—…ë°ì´íŠ¸ íšŸìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ìˆ˜ì • (65 epochë¶€í„° ì‹œì‘)
    global_update_step = START_EPOCH * len(dataloader) // cfg.GRAD_ACCUM_STEPS
    total_update_steps = TOTAL_EPOCHS * len(dataloader) // cfg.GRAD_ACCUM_STEPS
    print(f"Total update steps: {total_update_steps}, Starting from: {global_update_step}")
    
    optimizer.zero_grad()
    
    for epoch in range(START_EPOCH, TOTAL_EPOCHS):
        backbone.train()
        total_loss = 0
        current_loss_accum = 0
        nan_count = 0
        
        # LR ì—…ë°ì´íŠ¸
        current_lr = optimizer.param_groups[0]['lr']
        
        pbar = tqdm(dataloader, desc=f"Epoch {epoch+1}/{TOTAL_EPOCHS} (LR: {current_lr:.2e})")
        for step, (audio_mert, spec, target) in enumerate(pbar):
            audio_mert = audio_mert.to(device)
            spec = spec.to(device)
            target = target.to(device)
            
            # [ì¶”ê°€] ì…ë ¥ ë°ì´í„° NaN ì²´í¬
            if check_for_nan(audio_mert, "audio_mert") or check_for_nan(spec, "spec") or check_for_nan(target, "target"):
                print(f"Skipping batch {step} due to NaN in input")
                nan_count += 1
                continue
            
            # [ìˆ˜ì •] Progress ê³„ì‚°ì„ ì‹¤ì œ ì—…ë°ì´íŠ¸ ê¸°ì¤€ìœ¼ë¡œ ì¡°ì •
            progress = global_update_step / total_update_steps
            
            # [ë””ë²„ê¹…] ì²« ë²ˆì§¸ ë°°ì¹˜ì—ì„œ progress í™•ì¸
            if epoch == START_EPOCH and step == 0:
                print(f"\nğŸ” ì²« ë²ˆì§¸ ë°°ì¹˜ í™•ì¸:")
                print(f"   - global_update_step: {global_update_step}")
                print(f"   - total_update_steps: {total_update_steps}")
                print(f"   - progress: {progress:.4f}")
            
            # [ìˆ˜ì •] Mixed Precision Training - device_type ëª…ì‹œ
            with autocast(device_type='cuda'):
                loss = loss_wrapper(audio_mert, spec, target, progress)
                loss = loss / cfg.GRAD_ACCUM_STEPS
            
            # [ì¶”ê°€] Loss NaN ì²´í¬
            if torch.isnan(loss) or torch.isinf(loss):
                print(f"[WARNING] NaN/Inf loss at step {step}, skipping...")
                optimizer.zero_grad()
                scaler.update()  # scaler ìƒíƒœ ì—…ë°ì´íŠ¸
                nan_count += 1
                continue
            
            scaler.scale(loss).backward()
            current_loss_accum += loss.item()
            
            if (step + 1) % cfg.GRAD_ACCUM_STEPS == 0:
                # Gradient Clipping (í•™ìŠµ ì•ˆì •í™”)
                scaler.unscale_(optimizer)
                
                # [ì¶”ê°€] Gradient NaN ì²´í¬
                total_norm = torch.nn.utils.clip_grad_norm_(backbone.parameters(), MAX_GRAD_NORM)
                if torch.isnan(total_norm) or torch.isinf(total_norm):
                    print(f"[WARNING] NaN/Inf gradient norm at step {step}, skipping update...")
                    optimizer.zero_grad()
                    scaler.update()
                    current_loss_accum = 0
                    nan_count += 1
                    continue
                
                scaler.step(optimizer)
                scaler.update()
                optimizer.zero_grad()
                
                # [ìˆ˜ì •] ì‹¤ì œ ì—…ë°ì´íŠ¸ê°€ ì¼ì–´ë‚¬ì„ ë•Œë§Œ step ì¦ê°€
                global_update_step += 1
                
                # Logging (Restore original scale)
                pbar.set_postfix({
                    'loss': current_loss_accum * cfg.GRAD_ACCUM_STEPS, 
                    'lr': f"{current_lr:.2e}",
                    'prog': f"{progress:.3f}"
                })
                total_loss += current_loss_accum * cfg.GRAD_ACCUM_STEPS
                current_loss_accum = 0
        
        # Learning Rate Scheduler Step
        scheduler.step()
        
        # [ì¶”ê°€] Epoch ì¢…ë£Œ ì‹œ NaN ë°œìƒ íšŸìˆ˜ ì¶œë ¥
        if nan_count > 0:
            print(f"[WARNING] Epoch {epoch+1}: {nan_count} NaN/Inf occurrences")
            
        avg_loss = total_loss / max((len(dataloader) / cfg.GRAD_ACCUM_STEPS) - nan_count, 1)
        new_lr = optimizer.param_groups[0]['lr']
        print(f"ğŸ“Š Epoch {epoch+1} Avg Loss: {avg_loss:.4f}, LR: {new_lr:.2e}")
        
        # [ìˆ˜ì •] ë§¤ 5 ì—í­ë§ˆë‹¤ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (65ë¶€í„° ì‹œì‘í•˜ë¯€ë¡œ naming ì¡°ì •)
        if (epoch + 1) % 5 == 0:
            os.makedirs("checkpoints", exist_ok=True)
            save_path = f"checkpoints/n2n_from65_ep{epoch+1}.pth"
            
            # Unwrap DataParallel
            if isinstance(backbone, nn.DataParallel):
                state_dict = backbone.module.state_dict()
            else:
                state_dict = backbone.state_dict()
            
            # [ì¶”ê°€] ì €ì¥ ì „ NaN ì²´í¬
            has_nan = False
            for k, v in state_dict.items():
                if torch.isnan(v).any():
                    print(f"[ERROR] NaN in {k}, not saving checkpoint!")
                    has_nan = True
                    break
            
            if not has_nan:
                torch.save({
                    'model_state_dict': state_dict, 
                    'epoch': epoch+1,
                    'optimizer_state_dict': optimizer.state_dict(),
                    'scheduler_state_dict': scheduler.state_dict(),
                    'loss': avg_loss
                }, save_path)
                print(f"ğŸ’¾ Saved checkpoint to {save_path}")
            else:
                print(f"[ERROR] Checkpoint at epoch {epoch+1} has NaN, skipping save!")
    
    print(f"\nğŸ‰ Training completed! Final epoch: {TOTAL_EPOCHS}")

if __name__ == "__main__":
    main()